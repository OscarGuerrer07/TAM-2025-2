{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Parcial 2 ‚Äì NFL Big Data Bowl 2026 con TabNet\n\nEn este cuaderno trabajo el parcial usando la base de datos del concurso NFL Big Data Bowl 2026. La idea es tomar los datos crudos del concurso, preparar las variables que necesitamos, entrenar un modelo TabNet para predecir la posici√≥n final del jugador en el momento en que el bal√≥n llega (x e y) y, al final, generar el archivo `submission.csv` para subirlo a Kaggle.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:36:42.160299Z","iopub.execute_input":"2025-12-04T01:36:42.160564Z","iopub.status.idle":"2025-12-04T01:36:42.449064Z","shell.execute_reply.started":"2025-12-04T01:36:42.160540Z","shell.execute_reply":"2025-12-04T01:36:42.448503Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/officialpytorchtabnet/pytorch_tabnet-3.1.0-py3-none-any.whl\n/kaggle/input/officialpytorchtabnet/pytorch_tabnet-4.0-py3-none-any.whl\n/kaggle/input/officialpytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n/kaggle/input/officialpytorchtabnet/pytorch_tabnet-3.0.0-py3-none-any.whl\n/kaggle/input/nfl-big-data-bowl-2026-prediction/test_input.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/test.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/nfl_inference_server.py\n/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/nfl_gateway.py\n/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/__init__.py\n/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/templates.py\n/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/base_gateway.py\n/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/relay.py\n/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/kaggle_evaluation.proto\n/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/__init__.py\n/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/generated/__init__.py\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w17.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w05.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w10.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w03.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w18.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w05.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w11.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w12.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w16.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w06.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w18.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w10.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w02.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w08.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w12.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w13.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w15.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w03.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w13.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w15.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w16.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w01.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w04.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w14.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w14.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w09.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w01.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w07.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w11.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w06.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w04.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w09.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w17.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w07.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/input_2023_w08.csv\n/kaggle/input/nfl-big-data-bowl-2026-prediction/train/output_2023_w02.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Funciones base: datos, features y m√©tricas\n\nEn este bloque se dejo listas las funciones que vamos a usar en todo el cuaderno. Aqu√≠ est√°n las funciones que cargan los datos del concurso, las que acomodan la direcci√≥n de juego, las que construyen nuevas variables (velocidades, √°ngulos, distancias, etc.) y las que convierten las columnas categ√≥ricas en n√∫meros. Tambi√©n est√°n las funciones de evaluaci√≥n que uso m√°s adelante para medir qu√© tan buenas son las predicciones.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pickle\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# GPU CONFIGURATION & OPTIMIZATION\n# ============================================================================\n\ndef setup_gpu():\n    \"\"\"Configure GPU for optimal performance\"\"\"\n    print(\"=\"*80)\n    print(\"GPU CONFIGURATION\")\n    print(\"=\"*80)\n    \n    # Check available GPUs\n    gpus = tf.config.list_physical_devices('GPU')\n    print(f\"\\nüñ•Ô∏è  Available GPUs: {len(gpus)}\")\n    \n    if gpus:\n        try:\n            # Enable memory growth (don't allocate all GPU memory at once)\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n                print(f\"   ‚úì GPU: {gpu.name} - Memory growth enabled\")\n            \n            # Set GPU memory limit (optional - useful if sharing GPU)\n            # tf.config.set_logical_device_configuration(\n            #     gpus[0],\n            #     [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]  # 4GB\n            # )\n            \n            # Use mixed precision for faster training\n            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n            tf.keras.mixed_precision.set_global_policy(policy)\n            print(f\"   ‚úì Mixed precision enabled: {policy.name}\")\n            \n            logical_gpus = tf.config.list_logical_devices('GPU')\n            print(f\"   ‚úì Logical GPUs: {len(logical_gpus)}\")\n            \n        except RuntimeError as e:\n            print(f\"   ‚ö†Ô∏è  GPU configuration error: {e}\")\n    else:\n        print(\"   ‚ö†Ô∏è  No GPU found - using CPU (training will be slower)\")\n    \n    # Set TensorFlow options for better performance\n    tf.config.optimizer.set_jit(True)  # XLA compilation\n    print(\"   ‚úì XLA (Accelerated Linear Algebra) enabled\")\n    \n    print(f\"\\nüìä TensorFlow version: {tf.__version__}\")\n    print(f\"üìä Keras version: {keras.__version__}\")\n    \n    return len(gpus) > 0\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nCONFIG = {\n    'sequence_length': 10,\n    'max_frames_to_predict': 15,\n    'batch_size': 256,  # Larger batch for GPU\n    'epochs': 100,\n    'learning_rate': 0.001,\n    'validation_split': 0.15,\n    'use_gpu': True,\n}\n\n# ============================================================================\n# EVALUATION METRICS\n# ============================================================================\n\ndef calculate_rmse(y_true, y_pred):\n    \"\"\"Calculate Root Mean Squared Error\"\"\"\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n    return rmse\n\ndef calculate_mae(y_true, y_pred):\n    \"\"\"Calculate Mean Absolute Error\"\"\"\n    mae = np.mean(np.abs(y_true - y_pred))\n    return mae\n\ndef calculate_euclidean_distance(y_true, y_pred):\n    \"\"\"Calculate Euclidean distance between predicted and actual positions\"\"\"\n    distances = np.sqrt((y_true[:, 0] - y_pred[:, 0])**2 + \n                       (y_true[:, 1] - y_pred[:, 1])**2)\n    return distances\n\ndef evaluate_predictions(y_true, y_pred, split_name=\"Validation\"):\n    \"\"\"Comprehensive evaluation of predictions\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"üìä {split_name.upper()} SET EVALUATION\")\n    print(\"=\"*80)\n    \n    # Overall metrics\n    x_rmse = calculate_rmse(y_true[:, 0], y_pred[:, 0])\n    y_rmse = calculate_rmse(y_true[:, 1], y_pred[:, 1])\n    \n    x_mae = calculate_mae(y_true[:, 0], y_pred[:, 0])\n    y_mae = calculate_mae(y_true[:, 1], y_pred[:, 1])\n    \n    # Euclidean distance\n    distances = calculate_euclidean_distance(y_true, y_pred)\n    mean_distance = np.mean(distances)\n    median_distance = np.median(distances)\n    \n    print(f\"\\nüéØ POSITION ACCURACY:\")\n    print(f\"   X-coordinate:\")\n    print(f\"      RMSE: {x_rmse:.3f} yards\")\n    print(f\"      MAE:  {x_mae:.3f} yards\")\n    \n    print(f\"\\n   Y-coordinate:\")\n    print(f\"      RMSE: {y_rmse:.3f} yards\")\n    print(f\"      MAE:  {y_mae:.3f} yards\")\n    \n    print(f\"\\nüìè EUCLIDEAN DISTANCE:\")\n    print(f\"   Mean:   {mean_distance:.3f} yards\")\n    print(f\"   Median: {median_distance:.3f} yards\")\n    print(f\"   Std:    {np.std(distances):.3f} yards\")\n    print(f\"   Min:    {np.min(distances):.3f} yards\")\n    print(f\"   Max:    {np.max(distances):.3f} yards\")\n    \n    # Percentiles\n    print(f\"\\nüìä DISTANCE PERCENTILES:\")\n    for p in [25, 50, 75, 90, 95, 99]:\n        print(f\"   {p}th percentile: {np.percentile(distances, p):.3f} yards\")\n    \n    # Accuracy buckets\n    print(f\"\\nüéØ ACCURACY BUCKETS:\")\n    for threshold in [1, 2, 5, 10, 15, 20]:\n        within = (distances <= threshold).sum()\n        pct = 100 * within / len(distances)\n        print(f\"   Within {threshold:2d} yards: {within:6,} ({pct:5.2f}%)\")\n    \n    metrics = {\n        'x_rmse': x_rmse,\n        'y_rmse': y_rmse,\n        'x_mae': x_mae,\n        'y_mae': y_mae,\n        'mean_distance': mean_distance,\n        'median_distance': median_distance,\n        'distances': distances\n    }\n    \n    return metrics\n\ndef plot_predictions(y_true, y_pred, split_name=\"Validation\", save_path=\"predictions_plot.png\"):\n    \"\"\"Visualize predictions vs actual\"\"\"\n    \n    fig = plt.figure(figsize=(20, 12))\n    \n    # 1. X predictions scatter\n    ax1 = plt.subplot(2, 3, 1)\n    ax1.scatter(y_true[:, 0], y_pred[:, 0], alpha=0.3, s=1)\n    ax1.plot([0, 120], [0, 120], 'r--', linewidth=2)\n    ax1.set_xlabel('Actual X (yards)', fontsize=12)\n    ax1.set_ylabel('Predicted X (yards)', fontsize=12)\n    ax1.set_title(f'{split_name} - X Coordinate', fontsize=14, fontweight='bold')\n    ax1.grid(alpha=0.3)\n    \n    # 2. Y predictions scatter\n    ax2 = plt.subplot(2, 3, 2)\n    ax2.scatter(y_true[:, 1], y_pred[:, 1], alpha=0.3, s=1)\n    ax2.plot([0, 53.3], [0, 53.3], 'r--', linewidth=2)\n    ax2.set_xlabel('Actual Y (yards)', fontsize=12)\n    ax2.set_ylabel('Predicted Y (yards)', fontsize=12)\n    ax2.set_title(f'{split_name} - Y Coordinate', fontsize=14, fontweight='bold')\n    ax2.grid(alpha=0.3)\n    \n    # 3. Error distribution\n    ax3 = plt.subplot(2, 3, 3)\n    distances = calculate_euclidean_distance(y_true, y_pred)\n    ax3.hist(distances, bins=50, alpha=0.7, edgecolor='black')\n    ax3.axvline(np.mean(distances), color='red', linestyle='--', \n                linewidth=2, label=f'Mean: {np.mean(distances):.2f}')\n    ax3.set_xlabel('Euclidean Distance Error (yards)', fontsize=12)\n    ax3.set_ylabel('Frequency', fontsize=12)\n    ax3.set_title('Prediction Error Distribution', fontsize=14, fontweight='bold')\n    ax3.legend()\n    ax3.grid(alpha=0.3)\n    \n    # 4. X error distribution\n    ax4 = plt.subplot(2, 3, 4)\n    x_errors = y_true[:, 0] - y_pred[:, 0]\n    ax4.hist(x_errors, bins=50, alpha=0.7, edgecolor='black', color='green')\n    ax4.axvline(0, color='red', linestyle='--', linewidth=2)\n    ax4.set_xlabel('X Error (yards)', fontsize=12)\n    ax4.set_ylabel('Frequency', fontsize=12)\n    ax4.set_title(f'X Error - Mean: {np.mean(x_errors):.3f}', fontsize=14, fontweight='bold')\n    ax4.grid(alpha=0.3)\n    \n    # 5. Y error distribution\n    ax5 = plt.subplot(2, 3, 5)\n    y_errors = y_true[:, 1] - y_pred[:, 1]\n    ax5.hist(y_errors, bins=50, alpha=0.7, edgecolor='black', color='orange')\n    ax5.axvline(0, color='red', linestyle='--', linewidth=2)\n    ax5.set_xlabel('Y Error (yards)', fontsize=12)\n    ax5.set_ylabel('Frequency', fontsize=12)\n    ax5.set_title(f'Y Error - Mean: {np.mean(y_errors):.3f}', fontsize=14, fontweight='bold')\n    ax5.grid(alpha=0.3)\n    \n    # 6. Cumulative accuracy\n    ax6 = plt.subplot(2, 3, 6)\n    sorted_distances = np.sort(distances)\n    cumulative = np.arange(1, len(sorted_distances) + 1) / len(sorted_distances) * 100\n    ax6.plot(sorted_distances, cumulative, linewidth=2)\n    ax6.set_xlabel('Distance Threshold (yards)', fontsize=12)\n    ax6.set_ylabel('Cumulative % of Predictions', fontsize=12)\n    ax6.set_title('Cumulative Accuracy Curve', fontsize=14, fontweight='bold')\n    ax6.grid(alpha=0.3)\n    \n    # Add benchmarks\n    for threshold in [5, 10, 15]:\n        pct = (distances <= threshold).sum() / len(distances) * 100\n        ax6.axvline(threshold, linestyle='--', alpha=0.5)\n        ax6.text(threshold, pct, f'{pct:.1f}%', fontsize=10)\n    \n    plt.suptitle(f'{split_name} Set - Prediction Analysis', \n                 fontsize=16, fontweight='bold', y=0.995)\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    print(f\"\\n‚úì Saved plot: {save_path}\")\n    \n    return fig\n\ndef plot_training_history(history, save_path=\"training_history.png\"):\n    \"\"\"Plot training history\"\"\"\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Loss plot\n    axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n    axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n    axes[0].set_xlabel('Epoch', fontsize=12)\n    axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n    axes[0].legend()\n    axes[0].grid(alpha=0.3)\n    \n    # MAE plot\n    axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n    axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n    axes[1].set_xlabel('Epoch', fontsize=12)\n    axes[1].set_ylabel('MAE (yards)', fontsize=12)\n    axes[1].set_title('Training and Validation MAE', fontsize=14, fontweight='bold')\n    axes[1].legend()\n    axes[1].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    print(f\"‚úì Saved plot: {save_path}\")\n    \n    return fig\n\n# ============================================================================\n# IMPORT FUNCTIONS FROM ORIGINAL CODE\n# ============================================================================\n\ndef parse_height(height_str):\n    if pd.isna(height_str):\n        return np.nan\n    try:\n        feet, inches = map(int, str(height_str).split('-'))\n        return feet * 12 + inches\n    except:\n        return np.nan\n\ndef calculate_age(birth_date, reference_date='2023-09-01'):\n    try:\n        birth = pd.to_datetime(birth_date)\n        ref = pd.to_datetime(reference_date)\n        return (ref - birth).days / 365.25\n    except:\n        return np.nan\n\ndef load_training_data(data_path='/kaggle/input/nfl-big-data-bowl-2026-prediction/train'):\n    print(\"\\n\" + \"=\"*80)\n    print(\"LOADING TRAINING DATA\")\n    print(\"=\"*80)\n    \n    all_data = []\n    for week in range(1, 19):\n        file_path = f'{data_path}/input_2023_w{week:02d}.csv'\n        try:\n            df = pd.read_csv(file_path)\n            all_data.append(df)\n            print(f\"‚úì Week {week:02d}: {len(df):,} rows | {df['play_id'].nunique():,} plays\")\n        except FileNotFoundError:\n            print(f\"‚úó Week {week:02d}: File not found\")\n    \n    train_df = pd.concat(all_data, ignore_index=True)\n    print(f\"\\nTotal training data: {len(train_df):,} rows\")\n    print(f\" Unique plays: {(train_df['game_id'].astype(str) + '_' + train_df['play_id'].astype(str)).nunique():,}\")\n    print(f\"Players to predict: {train_df['player_to_predict'].sum():,}\")\n    \n    return train_df\n\ndef load_test_data():\n    print(\"\\n\" + \"=\"*80)\n    print(\"LOADING TEST DATA\")\n    print(\"=\"*80)\n    \n    test_input = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2026-prediction/test_input.csv')\n    test_targets = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2026-prediction/test.csv')\n    \n    print(f\"‚úì Test input: {len(test_input):,} rows\")\n    print(f\"‚úì Test targets: {len(test_targets):,} predictions needed\")\n    \n    return test_input, test_targets\n\ndef normalize_play_direction(df):\n    df = df.copy()\n    left_mask = df['play_direction'] == 'left'\n    num_flipped = left_mask.sum()\n    \n    df.loc[left_mask, 'x'] = 120 - df.loc[left_mask, 'x']\n    df.loc[left_mask, 'y'] = 53.3 - df.loc[left_mask, 'y']\n    df.loc[left_mask, 'dir'] = (df.loc[left_mask, 'dir'] + 180) % 360\n    df.loc[left_mask, 'o'] = (df.loc[left_mask, 'o'] + 180) % 360\n    \n    if 'ball_land_x' in df.columns:\n        df.loc[left_mask, 'ball_land_x'] = 120 - df.loc[left_mask, 'ball_land_x']\n        df.loc[left_mask, 'ball_land_y'] = 53.3 - df.loc[left_mask, 'ball_land_y']\n    \n    print(f\"   Normalized {num_flipped:,} plays moving left ‚Üí right\")\n    return df\n\ndef engineer_features(df):\n    print(\"\\n\" + \"=\"*80)\n    print(\"FEATURE ENGINEERING\")\n    print(\"=\"*80)\n    \n    df = df.copy()\n    \n    print(\"‚úì Computing velocity components (vx, vy)\")\n    df['vx'] = df['s'] * np.cos(np.radians(df['dir']))\n    df['vy'] = df['s'] * np.sin(np.radians(df['dir']))\n    \n    print(\"‚úì Computing orientation components (ox, oy)\")\n    df['ox'] = np.cos(np.radians(df['o']))\n    df['oy'] = np.sin(np.radians(df['o']))\n    \n    if 'ball_land_x' in df.columns:\n        print(\"‚úì Computing ball landing features\")\n        df['dist_to_ball'] = np.sqrt(\n            (df['x'] - df['ball_land_x'])**2 + \n            (df['y'] - df['ball_land_y'])**2\n        )\n        df['angle_to_ball'] = np.arctan2(\n            df['ball_land_y'] - df['y'],\n            df['ball_land_x'] - df['x']\n        )\n        df['vel_toward_ball'] = df['s'] * np.cos(np.radians(df['dir']) - df['angle_to_ball'])\n    else:\n        df['dist_to_ball'] = 0\n        df['angle_to_ball'] = 0\n        df['vel_toward_ball'] = 0\n    \n    print(\"‚úì Computing field position features\")\n    df['dist_to_left_sideline'] = df['y']\n    df['dist_to_right_sideline'] = 53.3 - df['y']\n    df['dist_to_nearest_sideline'] = np.minimum(df['y'], 53.3 - df['y'])\n    df['dist_to_endzone'] = 120 - df['x']\n    \n    print(\"‚úì Processing player attributes\")\n    df['height_inches'] = df['player_height'].apply(parse_height)\n    df['height_inches'] = df['height_inches'].fillna(df['height_inches'].median())\n    \n    df['player_age'] = df['player_birth_date'].apply(calculate_age)\n    df['player_age'] = df['player_age'].fillna(df['player_age'].median())\n    \n    df['bmi'] = (df['player_weight'] * 703) / (df['height_inches'] ** 2)\n    df['bmi'] = df['bmi'].fillna(df['bmi'].median())\n    \n    print(\"‚úì Creating temporal features (lags, differences)\")\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    \n    group_cols = ['game_id', 'play_id', 'nfl_id']\n    for lag in [1, 2, 3]:\n        for col in ['x', 'y', 's', 'a', 'vx', 'vy']:\n            df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n    \n    df['speed_change'] = df.groupby(group_cols)['s'].diff()\n    df['accel_change'] = df.groupby(group_cols)['a'].diff()\n    df['dir_change'] = df.groupby(group_cols)['dir'].diff()\n    \n    df.loc[df['dir_change'] > 180, 'dir_change'] -= 360\n    df.loc[df['dir_change'] < -180, 'dir_change'] += 360\n    \n    print(\"‚úì Computing rolling statistics\")\n    for col in ['s', 'a']:\n        df[f'{col}_roll_mean'] = df.groupby(group_cols)[col].transform(\n            lambda x: x.rolling(window=3, min_periods=1).mean()\n        )\n        df[f'{col}_roll_std'] = df.groupby(group_cols)[col].transform(\n            lambda x: x.rolling(window=3, min_periods=1).std()\n        )\n    \n    df = df.fillna(method='bfill').fillna(method='ffill').fillna(0)\n    \n    print(f\"\\nüìä Features created: {len(df.columns)} total columns\")\n    \n    return df\n\ndef encode_categorical(df, encoders=None):\n    df = df.copy()\n    categorical_cols = ['player_position', 'player_side', 'player_role']\n    \n    if encoders is None:\n        encoders = {}\n        for col in categorical_cols:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col].astype(str))\n            encoders[col] = le\n        return df, encoders\n    else:\n        for col in categorical_cols:\n            if col in encoders:\n                df[col] = df[col].astype(str).map(\n                    lambda x: x if x in encoders[col].classes_ else encoders[col].classes_[0]\n                )\n                df[col] = encoders[col].transform(df[col])\n        return df\n\ndef create_sequences(df, sequence_length=10, for_training=True):\n    print(\"\\n\" + \"=\"*80)\n    print(\"CREATING SEQUENCES\")\n    print(\"=\"*80)\n    \n    sequence_features = [\n        'x', 'y', 's', 'a', 'vx', 'vy', 'ox', 'oy', 'dir', 'o',\n        'x_lag1', 'y_lag1', 's_lag1', 'a_lag1',\n        'x_lag2', 'y_lag2', 's_lag2', 'a_lag2',\n        'x_lag3', 'y_lag3', 's_lag3', 'a_lag3',\n        'speed_change', 'accel_change', 'dir_change',\n        's_roll_mean', 'a_roll_mean',\n        'dist_to_left_sideline', 'dist_to_right_sideline', 'dist_to_nearest_sideline'\n    ]\n    \n    static_features = [\n        'player_position', 'player_side', 'player_role',\n        'height_inches', 'player_weight', 'player_age', 'bmi',\n        'absolute_yardline_number', 'dist_to_ball', 'angle_to_ball'\n    ]\n    \n    sequences = []\n    static_data = []\n    targets = []\n    metadata = []\n    \n    grouped = df.groupby(['game_id', 'play_id', 'nfl_id'])\n    \n    for (game_id, play_id, nfl_id), group in grouped:\n        if for_training and not group['player_to_predict'].any():\n            continue\n        \n        group = group.sort_values('frame_id')\n        \n        if len(group) < sequence_length:\n            continue\n        \n        seq_data = group[sequence_features].iloc[-sequence_length:].values\n        static = group[static_features].iloc[-1].values\n        \n        sequences.append(seq_data)\n        static_data.append(static)\n        \n        if for_training and 'ball_land_x' in group.columns:\n            target_x = group['ball_land_x'].iloc[-1]\n            target_y = group['ball_land_y'].iloc[-1]\n            targets.append([target_x, target_y])\n        \n        metadata.append({\n            'game_id': game_id,\n            'play_id': play_id,\n            'nfl_id': nfl_id,\n            'num_frames_output': group['num_frames_output'].iloc[-1] if 'num_frames_output' in group.columns else 0,\n            'last_x': group['x'].iloc[-1],\n            'last_y': group['y'].iloc[-1],\n        })\n    \n    sequences = np.array(sequences, dtype=np.float32)\n    static_data = np.array(static_data, dtype=np.float32)\n    \n    if for_training and len(targets) > 0:\n        targets = np.array(targets, dtype=np.float32)\n    else:\n        targets = None\n    \n    print(f\"‚úì Created {len(sequences):,} sequences\")\n    print(f\"‚úì Sequence shape: {sequences.shape}\")\n    print(f\"‚úì Static shape: {static_data.shape}\")\n    if targets is not None:\n        print(f\"‚úì Target shape: {targets.shape}\")\n    \n    return sequences, static_data, targets, metadata\n\ndef build_model(sequence_shape, static_shape):\n    print(\"\\n\" + \"=\"*80)\n    print(\"BUILDING MODEL\")\n    print(\"=\"*80)\n    \n    sequence_input = layers.Input(shape=sequence_shape, name='sequence_input')\n    \n    x = layers.LSTM(128, return_sequences=True)(sequence_input)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    x = layers.LSTM(64, return_sequences=False)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    static_input = layers.Input(shape=(static_shape,), name='static_input')\n    s = layers.Dense(64, activation='relu')(static_input)\n    s = layers.BatchNormalization()(s)\n    s = layers.Dropout(0.2)(s)\n    s = layers.Dense(32, activation='relu')(s)\n    \n    combined = layers.concatenate([x, s])\n    \n    z = layers.Dense(128, activation='relu')(combined)\n    z = layers.BatchNormalization()(z)\n    z = layers.Dropout(0.3)(z)\n    \n    z = layers.Dense(64, activation='relu')(z)\n    z = layers.Dropout(0.2)(z)\n    \n    # For mixed precision, use float32 output\n    output = layers.Dense(2, dtype='float32', name='position_output')(z)\n    \n    model = keras.Model(\n        inputs=[sequence_input, static_input],\n        outputs=output\n    )\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n        loss='mse',\n        metrics=['mae', 'mse']\n    )\n    \n    model.summary()\n    \n    return model\n\ndef train_model(model, X_seq, X_static, y, validation_split=0.15):\n    print(\"\\n\" + \"=\"*80)\n    print(\"TRAINING MODEL\")\n    print(\"=\"*80)\n    \n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=15,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=7,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        keras.callbacks.ModelCheckpoint(\n            'best_model.keras',\n            monitor='val_loss',\n            save_best_only=True,\n            verbose=1\n        )\n    ]\n    \n    history = model.fit(\n        [X_seq, X_static], y,\n        batch_size=CONFIG['batch_size'],\n        epochs=CONFIG['epochs'],\n        validation_split=validation_split,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    return model, history\n\ndef create_submission(model, test_input, test_targets, metadata_lookup, scalers):\n    print(\"\\n\" + \"=\"*80)\n    print(\"GENERATING PREDICTIONS\")\n    print(\"=\"*80)\n    \n    pred_dict = {}\n    for meta, pred in zip(metadata_lookup, model.predict([test_input[0], test_input[1]], verbose=1)):\n        key = (meta['game_id'], meta['play_id'], meta['nfl_id'])\n        pred_dict[key] = {\n            'x': pred[0],\n            'y': pred[1],\n            'last_x': meta['last_x'],\n            'last_y': meta['last_y']\n        }\n    \n    submissions = []\n    for _, row in test_targets.iterrows():\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        \n        if key in pred_dict:\n            x_pred = pred_dict[key]['x']\n            y_pred = pred_dict[key]['y']\n        else:\n            x_pred = 60.0\n            y_pred = 26.65\n        \n        submissions.append({\n            'id': f\"{row['game_id']}_{row['play_id']}_{row['nfl_id']}_{row['frame_id']}\",\n            'x': x_pred,\n            'y': y_pred\n        })\n    \n    submission_df = pd.DataFrame(submissions)\n    submission_df.to_csv('submission.csv', index=False)\n    \n    print(f\"‚úì Submission created: {len(submission_df):,} predictions\")\n    print(f\"‚úì Saved to: submission.csv\")\n    \n    return submission_df\n\n# ============================================================================\n# MAIN PIPELINE WITH EVALUATION\n# ============================================================================\n\n'''def main():\n    start_time = datetime.now()\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\" NFL BIG DATA BOWL 2026 - ENHANCED PIPELINE WITH EVALUATION\")\n    print(\"=\"*80)\n    \n    # Setup GPU\n    has_gpu = setup_gpu()\n    \n    # Load data\n    train_df = load_training_data()\n    test_input_df, test_targets_df = load_test_data()\n    \n    # Preprocess\n    print(\"\\nüìç Step 1: Normalizing play direction...\")\n    train_df = normalize_play_direction(train_df)\n    test_input_df = normalize_play_direction(test_input_df)\n    \n    # Feature engineering\n    print(\"\\nüìç Step 2: Feature engineering...\")\n    train_df = engineer_features(train_df)\n    test_input_df = engineer_features(test_input_df)\n    \n    # Encode categorical\n    print(\"\\nüìç Step 3: Encoding categorical variables...\")\n    train_df, encoders = encode_categorical(train_df)\n    test_input_df = encode_categorical(test_input_df, encoders)\n    \n    # Create sequences\n    print(\"\\nüìç Step 4: Creating sequences...\")\n    X_seq_all, X_static_all, y_all, metadata_all = create_sequences(\n        train_df, CONFIG['sequence_length'], for_training=True\n    )\n    \n    X_seq_test, X_static_test, _, metadata_test = create_sequences(\n        test_input_df, CONFIG['sequence_length'], for_training=False\n    )\n    \n    # Split train/validation\n    print(\"\\nüìç Step 5: Splitting train/validation...\")\n    n_samples = len(X_seq_all)\n    n_val = int(n_samples * CONFIG['validation_split'])\n    \n    # Random shuffle\n    indices = np.random.permutation(n_samples)\n    train_idx = indices[n_val:]\n    val_idx = indices[:n_val]\n    \n    X_seq_train = X_seq_all[train_idx]\n    X_static_train = X_static_all[train_idx]\n    y_train = y_all[train_idx]\n    \n    X_seq_val = X_seq_all[val_idx]\n    X_static_val = X_static_all[val_idx]\n    y_val = y_all[val_idx]\n    \n    print(f\"   Training samples: {len(X_seq_train):,}\")\n    print(f\"   Validation samples: {len(X_seq_val):,}\")\n    \n    # Scale features\n    print(\"\\nüìç Step 6: Scaling features...\")\n    scaler_seq = StandardScaler()\n    scaler_static = StandardScaler()\n    \n    # Scale sequence features\n    X_seq_train_flat = X_seq_train.reshape(-1, X_seq_train.shape[-1])\n    X_seq_train_scaled = scaler_seq.fit_transform(X_seq_train_flat).reshape(X_seq_train.shape)\n    \n    X_seq_val_flat = X_seq_val.reshape(-1, X_seq_val.shape[-1])\n    X_seq_val_scaled = scaler_seq.transform(X_seq_val_flat).reshape(X_seq_val.shape)\n    \n    # Scale static features\n    X_static_train_scaled = scaler_static.fit_transform(X_static_train)\n    X_static_val_scaled = scaler_static.transform(X_static_val)\n    \n    # Scale test features\n    X_seq_test_flat = X_seq_test.reshape(-1, X_seq_test.shape[-1])\n    X_seq_test_scaled = scaler_seq.transform(X_seq_test_flat).reshape(X_seq_test.shape)\n    X_static_test_scaled = scaler_static.transform(X_static_test)\n    \n    # Build model\n    print(\"\\nüìç Step 7: Building model...\")\n    model = build_model(\n        sequence_shape=(X_seq_train.shape[1], X_seq_train.shape[2]),\n        static_shape=X_static_train.shape[1]\n    )\n    \n    # Train model WITHOUT validation_split (we already split)\n    print(\"\\nüìç Step 8: Training model...\")\n    \n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=15,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=7,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        keras.callbacks.ModelCheckpoint(\n            'best_model.keras',\n            monitor='val_loss',\n            save_best_only=True,\n            verbose=1\n        )\n    ]\n    \n    history = model.fit(\n        [X_seq_train_scaled, X_static_train_scaled], y_train,\n        validation_data=([X_seq_val_scaled, X_static_val_scaled], y_val),\n        batch_size=CONFIG['batch_size'],\n        epochs=CONFIG['epochs'],\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Plot training history\n    print(\"\\nüìç Step 9: Plotting training history...\")\n    plot_training_history(history, \"training_history.png\")\n    \n    # Evaluate on training set\n    print(\"\\nüìç Step 10: Evaluating on training set...\")\n    y_train_pred = model.predict([X_seq_train_scaled, X_static_train_scaled], verbose=0)\n    train_metrics = evaluate_predictions(y_train, y_train_pred, \"Training\")\n    plot_predictions(y_train, y_train_pred, \"Training\", \"predictions_train.png\")\n    \n    # Evaluate on validation set\n    print(\"\\nüìç Step 11: Evaluating on validation set...\")\n    y_val_pred = model.predict([X_seq_val_scaled, X_static_val_scaled], verbose=0)\n    val_metrics = evaluate_predictions(y_val, y_val_pred, \"Validation\")\n    plot_predictions(y_val, y_val_pred, \"Validation\", \"predictions_val.png\")\n    \n    # Save model and artifacts\n    print(\"\\nüìç Step 12: Saving model and artifacts...\")\n    model.save('nfl_model_final.keras')\n    with open('scalers.pkl', 'wb') as f:\n        pickle.dump({'seq': scaler_seq, 'static': scaler_static, 'encoders': encoders}, f)\n    \n    # Save metrics to file\n    metrics_summary = {\n        'training': {\n            'x_rmse': float(train_metrics['x_rmse']),\n            'y_rmse': float(train_metrics['y_rmse']),\n            'x_mae': float(train_metrics['x_mae']),\n            'y_mae': float(train_metrics['y_mae']),\n            'mean_distance': float(train_metrics['mean_distance']),\n            'median_distance': float(train_metrics['median_distance'])\n        },\n        'validation': {\n            'x_rmse': float(val_metrics['x_rmse']),\n            'y_rmse': float(val_metrics['y_rmse']),\n            'x_mae': float(val_metrics['x_mae']),\n            'y_mae': float(val_metrics['y_mae']),\n            'mean_distance': float(val_metrics['mean_distance']),\n            'median_distance': float(val_metrics['median_distance'])\n        }\n    }\n    \n    with open('metrics.pkl', 'wb') as f:\n        pickle.dump(metrics_summary, f)\n    \n    # Create submission\n    print(\"\\nüìç Step 13: Creating submission...\")\n    submission = create_submission(\n        model, \n        (X_seq_test_scaled, X_static_test_scaled),\n        test_targets_df,\n        metadata_test,\n        {'seq': scaler_seq, 'static': scaler_static}\n    )\n    \n    # Final summary\n    end_time = datetime.now()\n    duration = end_time - start_time\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"‚úÖ PIPELINE COMPLETE!\")\n    print(\"=\"*80)\n    \n    print(f\"\\n‚è±Ô∏è  Total Time: {duration}\")\n    \n    print(f\"\\nüìÅ Files created:\")\n    print(f\"   ‚Ä¢ nfl_model_final.keras - Trained model\")\n    print(f\"   ‚Ä¢ best_model.keras - Best model checkpoint\")\n    print(f\"   ‚Ä¢ scalers.pkl - Feature scalers and encoders\")\n    print(f\"   ‚Ä¢ metrics.pkl - Evaluation metrics\")\n    print(f\"   ‚Ä¢ submission.csv - Final predictions ({len(submission):,} rows)\")\n    print(f\"   ‚Ä¢ training_history.png - Training curves\")\n    print(f\"   ‚Ä¢ predictions_train.png - Training set predictions\")\n    print(f\"   ‚Ä¢ predictions_val.png - Validation set predictions\")\n    \n    print(f\"\\nüìä FINAL RESULTS:\")\n    print(f\"\\n   Training Set:\")\n    print(f\"      RMSE (X): {train_metrics['x_rmse']:.3f} yards\")\n    print(f\"      RMSE (Y): {train_metrics['y_rmse']:.3f} yards\")\n    print(f\"      Mean Distance Error: {train_metrics['mean_distance']:.3f} yards\")\n    \n    print(f\"\\n   Validation Set:\")\n    print(f\"      RMSE (X): {val_metrics['x_rmse']:.3f} yards\")\n    print(f\"      RMSE (Y): {val_metrics['y_rmse']:.3f} yards\")\n    print(f\"      Mean Distance Error: {val_metrics['mean_distance']:.3f} yards\")\n    \n    print(f\"\\nüéØ Model Performance Summary:\")\n    within_5_val = (val_metrics['distances'] <= 5).sum() / len(val_metrics['distances']) * 100\n    within_10_val = (val_metrics['distances'] <= 10).sum() / len(val_metrics['distances']) * 100\n    print(f\"   Predictions within 5 yards: {within_5_val:.1f}%\")\n    print(f\"   Predictions within 10 yards: {within_10_val:.1f}%\")\n    \n    print(\"\\n\" + \"=\"*80)\n    \n    return model, history, submission, train_metrics, val_metrics'''\n\n'''# ============================================================================\n# RUN\n# ============================================================================\n\nif __name__ == \"__main__\":\n    model, history, submission, train_metrics, val_metrics = main()\n    '''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:36:42.450378Z","iopub.execute_input":"2025-12-04T01:36:42.450679Z","iopub.status.idle":"2025-12-04T01:36:55.628243Z","shell.execute_reply.started":"2025-12-04T01:36:42.450660Z","shell.execute_reply":"2025-12-04T01:36:55.627670Z"}},"outputs":[{"name":"stderr","text":"2025-12-04 01:36:44.387542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764812204.574825      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764812204.623875      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'# ============================================================================\\n# RUN\\n# ============================================================================\\n\\nif __name__ == \"__main__\":\\n    model, history, submission, train_metrics, val_metrics = main()\\n    '"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Parte 2 ‚Äì Cargar y normalizar los datos\n\nPrimero cargamos los datos oficiales del concurso: el conjunto de entrenamiento, y los archivos de test que Kaggle entrega para hacer la predicci√≥n. Despu√©s llamamos a la funci√≥n que normaliza la direcci√≥n de las jugadas, de modo que todas queden orientadas hacia el mismo lado del campo.\n\nHacer esto ayuda a que el modelo no tenga que aprender dos veces el mismo patr√≥n (una vez cuando el equipo ataca hacia la derecha y otra hacia la izquierda). B√°sicamente dejamos los datos un poco m√°s ordenados antes de empezar a crear features.\n","metadata":{}},{"cell_type":"code","source":"# ===== PARTE 2 ‚Äì Paso 1: Cargar y normalizar datos brutos =====\n\n# Cargar training set completo (todas las semanas disponibles en el dataset)\ntrain_df = load_training_data()\n\n# Cargar test_input (features de test) y test_targets (IDs + truth de Kaggle para evaluar offline)\ntest_input_df, test_targets_df = load_test_data()\n\nprint(\"\\nTama√±os originales de los DataFrames:\")\nprint(f\" train_df:       {train_df.shape}\")\nprint(f\" test_input_df:  {test_input_df.shape}\")\nprint(f\" test_targets_df:{test_targets_df.shape}\")\n\n# Normalizar la direcci√≥n de la jugada para que todas vayan de izquierda ‚ûú derecha\nprint(\"\\nNormalizando direcci√≥n de las jugadas (play_direction)...\")\ntrain_df = normalize_play_direction(train_df)\ntest_input_df = normalize_play_direction(test_input_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:36:55.628880Z","iopub.execute_input":"2025-12-04T01:36:55.629277Z","iopub.status.idle":"2025-12-04T01:37:17.117164Z","shell.execute_reply.started":"2025-12-04T01:36:55.629259Z","shell.execute_reply":"2025-12-04T01:37:17.116556Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nLOADING TRAINING DATA\n================================================================================\n‚úì Week 01: 285,714 rows | 748 plays\n‚úì Week 02: 288,586 rows | 777 plays\n‚úì Week 03: 297,757 rows | 823 plays\n‚úì Week 04: 272,475 rows | 710 plays\n‚úì Week 05: 254,779 rows | 677 plays\n‚úì Week 06: 270,676 rows | 715 plays\n‚úì Week 07: 233,597 rows | 646 plays\n‚úì Week 08: 281,011 rows | 765 plays\n‚úì Week 09: 252,796 rows | 656 plays\n‚úì Week 10: 260,372 rows | 673 plays\n‚úì Week 11: 243,413 rows | 657 plays\n‚úì Week 12: 294,940 rows | 755 plays\n‚úì Week 13: 233,755 rows | 622 plays\n‚úì Week 14: 279,972 rows | 738 plays\n‚úì Week 15: 281,820 rows | 702 plays\n‚úì Week 16: 316,417 rows | 822 plays\n‚úì Week 17: 277,582 rows | 734 plays\n‚úì Week 18: 254,917 rows | 686 plays\n\nTotal training data: 4,880,579 rows\n Unique plays: 14,108\nPlayers to predict: 1,303,440\n\n================================================================================\nLOADING TEST DATA\n================================================================================\n‚úì Test input: 49,753 rows\n‚úì Test targets: 5,837 predictions needed\n\nTama√±os originales de los DataFrames:\n train_df:       (4880579, 23)\n test_input_df:  (49753, 23)\n test_targets_df:(5837, 5)\n\nNormalizando direcci√≥n de las jugadas (play_direction)...\n   Normalized 2,421,505 plays moving left ‚Üí right\n   Normalized 22,819 plays moving left ‚Üí right\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Crear features y pasar todo a n√∫meros\n\nCon los datos crudos todav√≠a falta informaci√≥n √∫til para el modelo. En este paso usamos la funci√≥n de feature engineering para crear variables que describen mejor al jugador y a la jugada, por ejemplo cosas relacionadas con el movimiento, la posici√≥n en el campo y caracter√≠sticas f√≠sicas del jugador.\n\nLuego se convierte las columnas categ√≥ricas (posici√≥n, lado, rol, etc.) en valores num√©ricos. TabNet trabaja con tensores num√©ricos, as√≠ que aqu√≠ nos aseguramos de que todo el DataFrame quede en un formato que el modelo pueda usar directamente.","metadata":{}},{"cell_type":"code","source":"# ===== PARTE 2 ‚Äì Paso 2: Feature engineering + codificaci√≥n de categ√≥ricas =====\n\nprint(\"\\nAplicando feature engineering a train y test...\")\n\ntrain_df_fe = engineer_features(train_df)\ntest_input_df_fe = engineer_features(test_input_df)\n\nprint(\"\\nCodificando variables categ√≥ricas (player_position, player_side, player_role)...\")\ntrain_df_fe, encoders = encode_categorical(train_df_fe)\ntest_input_df_fe = encode_categorical(test_input_df_fe, encoders)\n\nprint(\"\\nShapes despu√©s de feature engineering + encoding:\")\nprint(f\" train_df_fe:      {train_df_fe.shape}\")\nprint(f\" test_input_df_fe: {test_input_df_fe.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T01:37:17.118474Z","iopub.execute_input":"2025-12-04T01:37:17.118668Z","iopub.status.idle":"2025-12-04T02:26:56.338201Z","shell.execute_reply.started":"2025-12-04T01:37:17.118653Z","shell.execute_reply":"2025-12-04T02:26:56.337546Z"}},"outputs":[{"name":"stdout","text":"\nAplicando feature engineering a train y test...\n\n================================================================================\nFEATURE ENGINEERING\n================================================================================\n‚úì Computing velocity components (vx, vy)\n‚úì Computing orientation components (ox, oy)\n‚úì Computing ball landing features\n‚úì Computing field position features\n‚úì Processing player attributes\n‚úì Creating temporal features (lags, differences)\n‚úì Computing rolling statistics\n\nüìä Features created: 62 total columns\n\n================================================================================\nFEATURE ENGINEERING\n================================================================================\n‚úì Computing velocity components (vx, vy)\n‚úì Computing orientation components (ox, oy)\n‚úì Computing ball landing features\n‚úì Computing field position features\n‚úì Processing player attributes\n‚úì Creating temporal features (lags, differences)\n‚úì Computing rolling statistics\n\nüìä Features created: 62 total columns\n\nCodificando variables categ√≥ricas (player_position, player_side, player_role)...\n\nShapes despu√©s de feature engineering + encoding:\n train_df_fe:      (4880579, 62)\n test_input_df_fe: (49753, 62)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Construir secuencias por jugador y jugada\n\nEl problema no es solo una foto est√°tica del jugador, sino c√≥mo se mueve mientras el bal√≥n est√° en el aire. Por eso, en este bloque agrupamos los datos por jugada y jugador, y construimos una secuencia de frames para cada uno, con una longitud fija.\n\nPara el conjunto de entrenamiento tambi√©n obtenemos el objetivo que quiero predecir, que son las coordenadas finales del jugador cuando termina la jugada. Para el test hacemos algo similar pero sin targets, y guardamos una metadata que luego nos sirve para armar correctamente el archivo de submission.","metadata":{}},{"cell_type":"code","source":"# ===== PARTE 2 ‚Äì Paso 3: Crear secuencias y convertirlas a features tabulares =====\n\n# Usaremos el mismo sequence_length que se defini√≥ en CONFIG\nSEQ_LEN = CONFIG['sequence_length']\nprint(f\"\\nUsando sequence_length = {SEQ_LEN} frames por muestra.\")\n\n# Crear secuencias para TRAIN (incluyen targets [ball_land_x, ball_land_y])\nX_seq_all, X_static_all, y_all, metadata_all = create_sequences(\n    train_df_fe,\n    sequence_length=SEQ_LEN,\n    for_training=True\n)\n\n# Crear secuencias para TEST (sin targets)\nX_seq_test, X_static_test, _, metadata_test = create_sequences(\n    test_input_df_fe,\n    sequence_length=SEQ_LEN,\n    for_training=False\n)\n\nprint(\"\\nShapes brutas de secuencias y est√°ticos:\")\nprint(f\" X_seq_all:       {X_seq_all.shape}\")\nprint(f\" X_static_all:    {X_static_all.shape}\")\nprint(f\" y_all:           {None if y_all is None else y_all.shape}\")\nprint(f\" X_seq_test:      {X_seq_test.shape}\")\nprint(f\" X_static_test:   {X_static_test.shape}\")\nprint(f\" #metadata_all:   {len(metadata_all)}\")\nprint(f\" #metadata_test:  {len(metadata_test)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:26:56.338900Z","iopub.execute_input":"2025-12-04T02:26:56.339166Z","iopub.status.idle":"2025-12-04T02:27:59.227762Z","shell.execute_reply.started":"2025-12-04T02:26:56.339138Z","shell.execute_reply":"2025-12-04T02:27:59.227165Z"}},"outputs":[{"name":"stdout","text":"\nUsando sequence_length = 10 frames por muestra.\n\n================================================================================\nCREATING SEQUENCES\n================================================================================\n‚úì Created 46,022 sequences\n‚úì Sequence shape: (46022, 10, 30)\n‚úì Static shape: (46022, 10)\n‚úì Target shape: (46022, 2)\n\n================================================================================\nCREATING SEQUENCES\n================================================================================\n‚úì Created 1,758 sequences\n‚úì Sequence shape: (1758, 10, 30)\n‚úì Static shape: (1758, 10)\n\nShapes brutas de secuencias y est√°ticos:\n X_seq_all:       (46022, 10, 30)\n X_static_all:    (46022, 10)\n y_all:           (46022, 2)\n X_seq_test:      (1758, 10, 30)\n X_static_test:   (1758, 10)\n #metadata_all:   46022\n #metadata_test:  1758\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Evitar fugas de informaci√≥n y pasar a formato tabular\n\nAqu√≠ hacemos dos ajustes importantes. Por un lado, eliminamos de las variables est√°ticas aquellas que se calculan usando la posici√≥n final del bal√≥n. Si dejara esas columnas, el modelo estar√≠a viendo informaci√≥n que en la pr√°ctica no deber√≠a conocer, y eso distorsiona el entrenamiento.\n\nPor otro lado, convertimos las secuencias en vectores largos y las unimos con las caracter√≠sticas est√°ticas \"seguras\". De esta forma terminamos con una matriz puramente tabular para entrenamiento y otra para test, que es justo el tipo de entrada que TabNet espera.\n","metadata":{}},{"cell_type":"code","source":"# ===== PARTE 2 ‚Äì Paso 3b: Limpiar est√°ticos y aplanar secuencias =====\n\n# 1) Eliminar de los est√°ticos las columnas con fuga de etiqueta:\n# static_features = [\n#     0:'player_position', 1:'player_side', 2:'player_role',\n#     3:'height_inches', 4:'player_weight', 5:'player_age', 6:'bmi',\n#     7:'absolute_yardline_number', 8:'dist_to_ball', 9:'angle_to_ball'\n# ]\nstatic_keep_idx = [0, 1, 2, 3, 4, 5, 6, 7]  # NOS quedamos hasta 'absolute_yardline_number'\n\nX_static_all_safe = X_static_all[:, static_keep_idx]\nX_static_test_safe = X_static_test[:, static_keep_idx]\n\nprint(\"\\nShapes de est√°ticos sin fuga de etiqueta:\")\nprint(f\" X_static_all_safe:  {X_static_all_safe.shape}\")\nprint(f\" X_static_test_safe: {X_static_test_safe.shape}\")\n\n# 2) Aplanar la parte secuencial: (N, SEQ_LEN, n_seq_features) -> (N, SEQ_LEN * n_seq_features)\nn_samples, seq_len, n_seq_features = X_seq_all.shape\nX_seq_all_flat = X_seq_all.reshape(n_samples, seq_len * n_seq_features)\n\nn_samples_test, seq_len_test, n_seq_features_test = X_seq_test.shape\nX_seq_test_flat = X_seq_test.reshape(n_samples_test, seq_len_test * n_seq_features_test)\n\nprint(\"\\nShapes de secuencias aplanadas:\")\nprint(f\" X_seq_all_flat:   {X_seq_all_flat.shape}\")\nprint(f\" X_seq_test_flat:  {X_seq_test_flat.shape}\")\n\n# 3) Concatenar parte secuencial + parte est√°tica segura para formar las features finales de TabNet\nimport numpy as np  # por si acaso no est√° en el namespace\n\nX_tabnet_all = np.hstack([X_seq_all_flat, X_static_all_safe])\nX_tabnet_test = np.hstack([X_seq_test_flat, X_static_test_safe])\n\n# El target es y_all (coordenadas de llegada del jugador, [ball_land_x, ball_land_y])\ny_tabnet_all = y_all\n\nprint(\"\\nFeatures finales para TabNet:\")\nprint(f\" X_tabnet_all:  {X_tabnet_all.shape}\")\nprint(f\" y_tabnet_all:  {y_tabnet_all.shape}\")\nprint(f\" X_tabnet_test: {X_tabnet_test.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:27:59.228543Z","iopub.execute_input":"2025-12-04T02:27:59.229312Z","iopub.status.idle":"2025-12-04T02:27:59.256889Z","shell.execute_reply.started":"2025-12-04T02:27:59.229291Z","shell.execute_reply":"2025-12-04T02:27:59.256092Z"}},"outputs":[{"name":"stdout","text":"\nShapes de est√°ticos sin fuga de etiqueta:\n X_static_all_safe:  (46022, 8)\n X_static_test_safe: (1758, 8)\n\nShapes de secuencias aplanadas:\n X_seq_all_flat:   (46022, 300)\n X_seq_test_flat:  (1758, 300)\n\nFeatures finales para TabNet:\n X_tabnet_all:  (46022, 308)\n y_tabnet_all:  (46022, 2)\n X_tabnet_test: (1758, 308)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Separar entrenamiento y validaci√≥n\n\nEn este paso dividimos los datos en dos grupos: uno para entrenar el modelo y otro para validarlo. El modelo solo ve el conjunto de entrenamiento durante el proceso de aprendizaje, y dejamos el conjunto de validaci√≥n aparte para comprobar qu√© tan bien generaliza con ejemplos que no ha visto antes.\n\nEste split es la base para todas las m√©tricas que muestra m√°s adelante y tambi√©n es lo que uso dentro de Optuna para decidir qu√© combinaci√≥n de hiperpar√°metros funciona mejor.\n","metadata":{}},{"cell_type":"code","source":"# ===== PARTE 2 ‚Äì Paso 4: Separar train y validaci√≥n para TabNet =====\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_tabnet_all,\n    y_tabnet_all,\n    test_size=0.2,      # 20% para validaci√≥n\n    random_state=42\n)\n\nprint(\"\\nSplit train/valid para TabNet:\")\nprint(f\" X_train: {X_train.shape}\")\nprint(f\" y_train: {y_train.shape}\")\nprint(f\" X_valid: {X_valid.shape}\")\nprint(f\" y_valid: {y_valid.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:27:59.257717Z","iopub.execute_input":"2025-12-04T02:27:59.258342Z","iopub.status.idle":"2025-12-04T02:27:59.287490Z","shell.execute_reply.started":"2025-12-04T02:27:59.258322Z","shell.execute_reply":"2025-12-04T02:27:59.286749Z"}},"outputs":[{"name":"stdout","text":"\nSplit train/valid para TabNet:\n X_train: (36817, 308)\n y_train: (36817, 2)\n X_valid: (9205, 308)\n y_valid: (9205, 2)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Parte 3 ‚Äì Ajustar los datos para TabNet\n\nAntes de crear el modelo, convertimos las matrices de entrada y de salida a tipo `float32`. Esto es un detalle que b√°sicamente es el tipo de dato que espera TabNet (que est√° implementado en PyTorch) y adem√°s ayuda a ahorrar algo de memoria.\n\nNo cambia la informaci√≥n, solo la forma en la que se almacena internamente.","metadata":{}},{"cell_type":"code","source":"# ===== PARTE 3 ‚Äì Paso 0: Preparar matrices en float32 para TabNet =====\n\n# TabNet (versi√≥n PyTorch) trabaja mejor con float32\nX_train_tab = X_train.astype(np.float32)\ny_train_tab = y_train.astype(np.float32)\nX_valid_tab = X_valid.astype(np.float32)\ny_valid_tab = y_valid.astype(np.float32)\nX_test_tab  = X_tabnet_test.astype(np.float32)\n\nprint(\"Dtypes -> X_train_tab:\", X_train_tab.dtype, \" y_train_tab:\", y_train_tab.dtype)\nprint(\"Shapes -> X_train_tab:\", X_train_tab.shape, \" X_valid_tab:\", X_valid_tab.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:02:29.046074Z","iopub.execute_input":"2025-12-04T03:02:29.046371Z","iopub.status.idle":"2025-12-04T03:02:29.075934Z","shell.execute_reply.started":"2025-12-04T03:02:29.046348Z","shell.execute_reply":"2025-12-04T03:02:29.075273Z"}},"outputs":[{"name":"stdout","text":"Dtypes -> X_train_tab: float32  y_train_tab: float32\nShapes -> X_train_tab: (36817, 308)  X_valid_tab: (9205, 308)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### Traer TabNet y configurar Optuna\n\nEn esta parte ya entramos en el modelo como tal. Primero instalamos e importamos TabNet y Optuna. TabNet es el modelo que vamos a usar para predecir `x` e `y` a partir de las features tabulares que construimos, y Optuna es la herramienta que me ayuda a probar distintas combinaciones de hiperpar√°metros sin hacerlo a mano.\n\nDespu√©s definimos la funci√≥n `objective`, que es la que Optuna va a llamar varias veces. Cada vez que se ejecuta, arma un TabNet con ciertos par√°metros, lo entrena unas cuantas √©pocas y mide el error en validaci√≥n usando la distancia entre la posici√≥n real y la predicha. Con base en ese valor Optuna decide qu√© configuraciones probar y cu√°l es la mejor.\n","metadata":{}},{"cell_type":"code","source":"# ===== PARTE 3 ‚Äì Paso 1: Instalar e importar TabNet + Optuna =====\n\n# Instalaci√≥n (solo es necesario la primera vez; luego puedes comentarlo si quieres)\n!pip install pytorch-tabnet optuna -q\n\nimport torch\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport optuna\n\n# Por reproducibilidad\ntorch.manual_seed(42)\nnp.random.seed(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:39:11.381975Z","iopub.execute_input":"2025-12-04T03:39:11.382530Z","iopub.status.idle":"2025-12-04T03:40:19.590772Z","shell.execute_reply.started":"2025-12-04T03:39:11.382505Z","shell.execute_reply":"2025-12-04T03:40:19.590030Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"'''\n# ===== PARTE 3 ‚Äì Paso 2: Definir funci√≥n objetivo para Optuna =====\n\n# Para que esta etapa sea razonablemente r√°pida,\n# usamos solo una muestra de X_train_tab / y_train_tab durante la b√∫squeda.\nMAX_TUNING_SAMPLES = 60000  # luego puedes ajustar este valor\n\nn_train = X_train_tab.shape[0]\ntuning_size = min(MAX_TUNING_SAMPLES, n_train)\n\nX_tune = X_train_tab[:tuning_size]\ny_tune = y_train_tab[:tuning_size]\n\nprint(f\"Usando {tuning_size} muestras de entrenamiento para la b√∫squeda de hiperpar√°metros.\")\n\ndef objective(trial):\n    # Hiperpar√°metros a optimizar\n    n_steps = trial.suggest_int(\"n_steps\", 3, 8)\n    lambda_sparse = trial.suggest_loguniform(\"lambda_sparse\", 1e-5, 1e-1)\n\n    # Otros par√°metros de TabNet (puedes ajustarlos si el profe dio valores espec√≠ficos)\n    n_d = trial.suggest_int(\"n_d\", 16, 64, step=16)  # tama√±o de las capas de decisi√≥n\n    n_a = n_d  # tama√±o de las capas de atenci√≥n, igual que n_d\n\n    model = TabNetRegressor(\n        n_d=n_d,\n        n_a=n_a,\n        n_steps=n_steps,\n        gamma=1.5,\n        lambda_sparse=lambda_sparse,\n        optimizer_fn=torch.optim.Adam,\n        optimizer_params=dict(lr=1e-3),\n        mask_type=\"sparsemax\",\n        verbose=0\n    )\n\n    # Par√°metros de entrenamiento reducidos para desarrollo\n    MAX_EPOCHS = 25      # m√°s adelante los subimos\n    BATCH_SIZE = 1024\n\n    model.fit(\n        X_tune, y_tune,\n        eval_set=[(X_valid_tab, y_valid_tab)],\n        eval_name=[\"valid\"],\n        eval_metric=[\"rmse\"],\n        max_epochs=MAX_EPOCHS,\n        patience=5,\n        batch_size=BATCH_SIZE,\n        virtual_batch_size=128,\n        num_workers=0,\n        drop_last=False\n    )\n\n   # Predicciones en validaci√≥n\n    y_pred_valid = model.predict(X_valid_tab)\n\n    # M√©trica basada en distancia euclidiana promedio (m√°s bajo es mejor)\n    dists = calculate_euclidean_distance(y_valid_tab, y_pred_valid)\n    mean_dist = float(np.mean(dists))\n\n    # Guardamos la m√©trica en los atributos del trial para poder verla luego\n    trial.set_user_attr(\"mean_valid_distance\", mean_dist)\n\n    return mean_dist\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:28:02.921601Z","iopub.status.idle":"2025-12-04T02:28:02.921834Z","shell.execute_reply.started":"2025-12-04T02:28:02.921724Z","shell.execute_reply":"2025-12-04T02:28:02.921734Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### B√∫squeda de hiperpar√°metros\n\nAqu√≠ ejecutamos el estudio de Optuna con un n√∫mero de pruebas definido. Cada prueba corresponde a un modelo TabNet con una configuraci√≥n diferente de hiperpar√°metros. Al final, Optuna nos devuelve cu√°les valores funcionaron mejor seg√∫n la m√©trica de validaci√≥n.\n\nEsos hiperpar√°metros se guardan y son los que usamos para construir el modelo final que vamos a entrenar con m√°s calma en la siguiente parte.\n","metadata":{}},{"cell_type":"code","source":"'''\n# ===== PARTE 3 ‚Äì Paso 3: Ejecutar la b√∫squeda de hiperpar√°metros con Optuna =====\n\nN_TRIALS = 20  # durante el desarrollo; luego puedes aumentarlo\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=N_TRIALS)\n\nprint(\"\\nMejores hiperpar√°metros encontrados:\")\nprint(study.best_trial.params)\nprint(\"Mejor distancia euclidiana media en validaci√≥n:\",\n      study.best_trial.user_attrs[\"mean_valid_distance\"])\n\nbest_params = study.best_trial.params\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T02:28:02.923930Z","iopub.status.idle":"2025-12-04T02:28:02.924188Z","shell.execute_reply.started":"2025-12-04T02:28:02.924059Z","shell.execute_reply":"2025-12-04T02:28:02.924069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== PARTE 3 ‚Äì Paso 3: Hiperpar√°metros fijos de TabNet =====\n\nbest_params = {\n    \"n_steps\": 3,\n    \"lambda_sparse\": 0.003694302625583284,\n    \"n_d\": 48,\n}\n\nprint(\"Usando hiperpar√°metros fijos obtenidos previamente con Optuna (fuera de este notebook):\")\nfor k, v in best_params.items():\n    print(f\"  {k} = {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:40:47.633874Z","iopub.execute_input":"2025-12-04T03:40:47.634174Z","iopub.status.idle":"2025-12-04T03:40:47.638882Z","shell.execute_reply.started":"2025-12-04T03:40:47.634144Z","shell.execute_reply":"2025-12-04T03:40:47.638218Z"}},"outputs":[{"name":"stdout","text":"Usando hiperpar√°metros fijos obtenidos previamente con Optuna (fuera de este notebook):\n  n_steps = 3\n  lambda_sparse = 0.003694302625583284\n  n_d = 48\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Entrenar el modelo final con los mejores par√°metros\n\nCon los valores que encontr√≥ Optuna ya puedemos definir el modelo final de TabNet. Uso esos hiperpar√°metros para crear el modelo y luego lo entreno con los datos de entrenamiento, mientras seguimos revisando el error en el conjunto de validaci√≥n.\n\nLa idea es que, en lugar de entrenar cualquier configuraci√≥n, aqu√≠ ya usamos una que tiene sentido para este problema, y que fue seleccionada precisamente porque dio un buen desempe√±o en los datos de validaci√≥n.\n","metadata":{}},{"cell_type":"code","source":"# ===== PARTE 4 ‚Äì Paso 1: Definir el modelo final de TabNet con los mejores hiperpar√°metros =====\n\nbest_n_steps = best_params[\"n_steps\"]\nbest_lambda_sparse = best_params[\"lambda_sparse\"]\nbest_n_d = best_params[\"n_d\"]\nbest_n_a = best_n_d  # solemos usar n_a = n_d\n\nprint(\"Mejores hiperpar√°metros para el modelo final:\")\nprint(f\"  n_steps       = {best_n_steps}\")\nprint(f\"  lambda_sparse = {best_lambda_sparse}\")\nprint(f\"  n_d, n_a      = {best_n_d}\")\n\nfinal_model = TabNetRegressor(\n    n_d=best_n_d,\n    n_a=best_n_a,\n    n_steps=best_n_steps,\n    gamma=1.5,\n    lambda_sparse=best_lambda_sparse,\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=1e-3),\n    mask_type=\"sparsemax\",\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:45:21.741224Z","iopub.execute_input":"2025-12-04T03:45:21.741484Z","iopub.status.idle":"2025-12-04T03:45:21.806502Z","shell.execute_reply.started":"2025-12-04T03:45:21.741465Z","shell.execute_reply":"2025-12-04T03:45:21.805754Z"}},"outputs":[{"name":"stdout","text":"Mejores hiperpar√°metros para el modelo final:\n  n_steps       = 3\n  lambda_sparse = 0.003694302625583284\n  n_d, n_a      = 48\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ===== PARTE 4 ‚Äì Paso 2: Entrenar el modelo final (por ahora con pocos epochs) =====\n\nMAX_EPOCHS_FINAL = 150   \nBATCH_SIZE_FINAL = 2048  # batch grande para ir m√°s r√°pido\n\nfinal_model.fit(\n    X_train_tab, y_train_tab,\n    eval_set=[(X_valid_tab, y_valid_tab)],\n    eval_name=[\"valid\"],\n    eval_metric=[\"rmse\"],\n    max_epochs=MAX_EPOCHS_FINAL,\n    patience=10,\n    batch_size=BATCH_SIZE_FINAL,\n    virtual_batch_size=256,\n    num_workers=0,\n    drop_last=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:45:28.803371Z","iopub.execute_input":"2025-12-04T03:45:28.804298Z","iopub.status.idle":"2025-12-04T03:48:12.968356Z","shell.execute_reply.started":"2025-12-04T03:45:28.804256Z","shell.execute_reply":"2025-12-04T03:48:12.967532Z"}},"outputs":[{"name":"stdout","text":"epoch 0  | loss: 2849.19587| valid_rmse: 43.80984115600586|  0:00:01s\nepoch 1  | loss: 2706.20674| valid_rmse: 42.120418548583984|  0:00:02s\nepoch 2  | loss: 2560.20397| valid_rmse: 43.05683898925781|  0:00:03s\nepoch 3  | loss: 2408.42646| valid_rmse: 42.162879943847656|  0:00:04s\nepoch 4  | loss: 2243.985| valid_rmse: 41.3678092956543|  0:00:06s\nepoch 5  | loss: 2075.42129| valid_rmse: 40.30149841308594|  0:00:07s\nepoch 6  | loss: 1896.41846| valid_rmse: 38.74258041381836|  0:00:08s\nepoch 7  | loss: 1701.75818| valid_rmse: 36.229949951171875|  0:00:09s\nepoch 8  | loss: 1499.78464| valid_rmse: 33.75571823120117|  0:00:10s\nepoch 9  | loss: 1291.50736| valid_rmse: 30.810359954833984|  0:00:11s\nepoch 10 | loss: 1084.97862| valid_rmse: 27.621980667114258|  0:00:12s\nepoch 11 | loss: 889.00873| valid_rmse: 24.670040130615234|  0:00:13s\nepoch 12 | loss: 708.48612| valid_rmse: 22.109600067138672|  0:00:14s\nepoch 13 | loss: 548.20943| valid_rmse: 19.55084991455078|  0:00:15s\nepoch 14 | loss: 413.05378| valid_rmse: 17.09210968017578|  0:00:16s\nepoch 15 | loss: 295.52841| valid_rmse: 14.274669647216797|  0:00:17s\nepoch 16 | loss: 204.76568| valid_rmse: 12.177330017089844|  0:00:18s\nepoch 17 | loss: 143.00834| valid_rmse: 10.710840225219727|  0:00:20s\nepoch 18 | loss: 108.34623| valid_rmse: 9.897250175476074|  0:00:21s\nepoch 19 | loss: 91.24647| valid_rmse: 9.336759567260742|  0:00:22s\nepoch 20 | loss: 84.01966| valid_rmse: 8.979789733886719|  0:00:23s\nepoch 21 | loss: 77.42873| valid_rmse: 8.670780181884766|  0:00:24s\nepoch 22 | loss: 72.76715| valid_rmse: 8.48723030090332|  0:00:25s\nepoch 23 | loss: 69.63251| valid_rmse: 8.377479553222656|  0:00:26s\nepoch 24 | loss: 68.11412| valid_rmse: 8.20592975616455|  0:00:27s\nepoch 25 | loss: 66.75666| valid_rmse: 8.082260131835938|  0:00:28s\nepoch 26 | loss: 64.69473| valid_rmse: 8.019479751586914|  0:00:29s\nepoch 27 | loss: 63.52989| valid_rmse: 7.904930114746094|  0:00:30s\nepoch 28 | loss: 62.06619| valid_rmse: 7.845870018005371|  0:00:31s\nepoch 29 | loss: 61.08383| valid_rmse: 7.788219928741455|  0:00:32s\nepoch 30 | loss: 60.77709| valid_rmse: 7.770380020141602|  0:00:34s\nepoch 31 | loss: 59.63186| valid_rmse: 7.693860054016113|  0:00:35s\nepoch 32 | loss: 59.11115| valid_rmse: 7.66234016418457|  0:00:36s\nepoch 33 | loss: 58.16319| valid_rmse: 7.576869964599609|  0:00:37s\nepoch 34 | loss: 57.37986| valid_rmse: 7.53957986831665|  0:00:38s\nepoch 35 | loss: 56.55819| valid_rmse: 7.506380081176758|  0:00:39s\nepoch 36 | loss: 56.47912| valid_rmse: 7.49314022064209|  0:00:40s\nepoch 37 | loss: 55.88708| valid_rmse: 7.459849834442139|  0:00:41s\nepoch 38 | loss: 55.41159| valid_rmse: 7.418900012969971|  0:00:42s\nepoch 39 | loss: 55.33565| valid_rmse: 7.41903018951416|  0:00:43s\nepoch 40 | loss: 54.80788| valid_rmse: 7.394529819488525|  0:00:44s\nepoch 41 | loss: 54.35615| valid_rmse: 7.362100124359131|  0:00:45s\nepoch 42 | loss: 53.71604| valid_rmse: 7.364729881286621|  0:00:46s\nepoch 43 | loss: 53.95271| valid_rmse: 7.349599838256836|  0:00:48s\nepoch 44 | loss: 53.37569| valid_rmse: 7.299069881439209|  0:00:49s\nepoch 45 | loss: 52.86866| valid_rmse: 7.2955498695373535|  0:00:50s\nepoch 46 | loss: 52.29356| valid_rmse: 7.231790065765381|  0:00:51s\nepoch 47 | loss: 51.7488 | valid_rmse: 7.245560169219971|  0:00:52s\nepoch 48 | loss: 51.98116| valid_rmse: 7.217299938201904|  0:00:53s\nepoch 49 | loss: 51.36941| valid_rmse: 7.194369792938232|  0:00:54s\nepoch 50 | loss: 50.99539| valid_rmse: 7.151559829711914|  0:00:55s\nepoch 51 | loss: 50.88677| valid_rmse: 7.165150165557861|  0:00:56s\nepoch 52 | loss: 50.80297| valid_rmse: 7.135580062866211|  0:00:57s\nepoch 53 | loss: 50.23419| valid_rmse: 7.120480060577393|  0:00:58s\nepoch 54 | loss: 50.03031| valid_rmse: 7.08814001083374|  0:00:59s\nepoch 55 | loss: 50.02033| valid_rmse: 7.08450984954834|  0:01:00s\nepoch 56 | loss: 49.12156| valid_rmse: 7.064509868621826|  0:01:01s\nepoch 57 | loss: 49.41414| valid_rmse: 7.039400100708008|  0:01:02s\nepoch 58 | loss: 49.10326| valid_rmse: 7.005730152130127|  0:01:03s\nepoch 59 | loss: 48.72652| valid_rmse: 6.980100154876709|  0:01:05s\nepoch 60 | loss: 48.73718| valid_rmse: 6.982999801635742|  0:01:06s\nepoch 61 | loss: 48.30889| valid_rmse: 6.962039947509766|  0:01:07s\nepoch 62 | loss: 47.82364| valid_rmse: 6.959290027618408|  0:01:08s\nepoch 63 | loss: 47.94737| valid_rmse: 6.989709854125977|  0:01:09s\nepoch 64 | loss: 47.79596| valid_rmse: 6.941410064697266|  0:01:10s\nepoch 65 | loss: 47.81011| valid_rmse: 6.958769798278809|  0:01:11s\nepoch 66 | loss: 47.37793| valid_rmse: 6.959139823913574|  0:01:12s\nepoch 67 | loss: 47.19111| valid_rmse: 6.936960220336914|  0:01:13s\nepoch 68 | loss: 46.87735| valid_rmse: 6.923709869384766|  0:01:14s\nepoch 69 | loss: 46.75366| valid_rmse: 6.930369853973389|  0:01:15s\nepoch 70 | loss: 46.53169| valid_rmse: 6.921629905700684|  0:01:16s\nepoch 71 | loss: 46.23078| valid_rmse: 6.892129898071289|  0:01:17s\nepoch 72 | loss: 46.28763| valid_rmse: 6.876120090484619|  0:01:18s\nepoch 73 | loss: 45.87964| valid_rmse: 6.862639904022217|  0:01:19s\nepoch 74 | loss: 45.96697| valid_rmse: 6.861519813537598|  0:01:20s\nepoch 75 | loss: 45.51915| valid_rmse: 6.815659999847412|  0:01:21s\nepoch 76 | loss: 45.63578| valid_rmse: 6.841559886932373|  0:01:23s\nepoch 77 | loss: 45.36534| valid_rmse: 6.853630065917969|  0:01:24s\nepoch 78 | loss: 45.25214| valid_rmse: 6.846439838409424|  0:01:25s\nepoch 79 | loss: 45.05374| valid_rmse: 6.801109790802002|  0:01:26s\nepoch 80 | loss: 44.99551| valid_rmse: 6.80118989944458|  0:01:27s\nepoch 81 | loss: 44.62488| valid_rmse: 6.769110202789307|  0:01:28s\nepoch 82 | loss: 44.97681| valid_rmse: 6.774370193481445|  0:01:29s\nepoch 83 | loss: 44.70971| valid_rmse: 6.774590015411377|  0:01:30s\nepoch 84 | loss: 44.4399 | valid_rmse: 6.756720066070557|  0:01:31s\nepoch 85 | loss: 44.20476| valid_rmse: 6.772250175476074|  0:01:32s\nepoch 86 | loss: 44.26393| valid_rmse: 6.773200035095215|  0:01:33s\nepoch 87 | loss: 44.18885| valid_rmse: 6.765140056610107|  0:01:34s\nepoch 88 | loss: 43.84324| valid_rmse: 6.7528300285339355|  0:01:35s\nepoch 89 | loss: 43.58684| valid_rmse: 6.741519927978516|  0:01:36s\nepoch 90 | loss: 43.81437| valid_rmse: 6.736599922180176|  0:01:37s\nepoch 91 | loss: 43.64851| valid_rmse: 6.713220119476318|  0:01:38s\nepoch 92 | loss: 43.34169| valid_rmse: 6.722539901733398|  0:01:39s\nepoch 93 | loss: 43.12453| valid_rmse: 6.688920021057129|  0:01:40s\nepoch 94 | loss: 42.8029 | valid_rmse: 6.673769950866699|  0:01:42s\nepoch 95 | loss: 42.63236| valid_rmse: 6.695000171661377|  0:01:43s\nepoch 96 | loss: 42.4983 | valid_rmse: 6.686339855194092|  0:01:44s\nepoch 97 | loss: 42.74376| valid_rmse: 6.7154998779296875|  0:01:45s\nepoch 98 | loss: 42.47696| valid_rmse: 6.713799953460693|  0:01:46s\nepoch 99 | loss: 42.65328| valid_rmse: 6.68218994140625|  0:01:47s\nepoch 100| loss: 42.32147| valid_rmse: 6.686550140380859|  0:01:48s\nepoch 101| loss: 42.06615| valid_rmse: 6.659540176391602|  0:01:49s\nepoch 102| loss: 42.44105| valid_rmse: 6.66441011428833|  0:01:50s\nepoch 103| loss: 41.71952| valid_rmse: 6.644949913024902|  0:01:51s\nepoch 104| loss: 41.65005| valid_rmse: 6.626619815826416|  0:01:52s\nepoch 105| loss: 41.55714| valid_rmse: 6.641749858856201|  0:01:53s\nepoch 106| loss: 41.50545| valid_rmse: 6.650780200958252|  0:01:54s\nepoch 107| loss: 41.42068| valid_rmse: 6.6526198387146|  0:01:55s\nepoch 108| loss: 41.16803| valid_rmse: 6.610640048980713|  0:01:56s\nepoch 109| loss: 41.41913| valid_rmse: 6.623589992523193|  0:01:57s\nepoch 110| loss: 41.15636| valid_rmse: 6.616020202636719|  0:01:59s\nepoch 111| loss: 40.68837| valid_rmse: 6.606760025024414|  0:02:00s\nepoch 112| loss: 40.60563| valid_rmse: 6.623720169067383|  0:02:01s\nepoch 113| loss: 40.54351| valid_rmse: 6.6079301834106445|  0:02:02s\nepoch 114| loss: 40.41899| valid_rmse: 6.629570007324219|  0:02:03s\nepoch 115| loss: 40.5563 | valid_rmse: 6.611649990081787|  0:02:04s\nepoch 116| loss: 40.24616| valid_rmse: 6.594729900360107|  0:02:05s\nepoch 117| loss: 40.35163| valid_rmse: 6.624959945678711|  0:02:06s\nepoch 118| loss: 39.90685| valid_rmse: 6.61614990234375|  0:02:07s\nepoch 119| loss: 40.27349| valid_rmse: 6.617720127105713|  0:02:08s\nepoch 120| loss: 40.21305| valid_rmse: 6.607639789581299|  0:02:09s\nepoch 121| loss: 40.08831| valid_rmse: 6.602059841156006|  0:02:10s\nepoch 122| loss: 39.91376| valid_rmse: 6.588980197906494|  0:02:11s\nepoch 123| loss: 40.16627| valid_rmse: 6.609119892120361|  0:02:12s\nepoch 124| loss: 40.01433| valid_rmse: 6.584670066833496|  0:02:13s\nepoch 125| loss: 39.53394| valid_rmse: 6.587709903717041|  0:02:14s\nepoch 126| loss: 39.57282| valid_rmse: 6.583539962768555|  0:02:15s\nepoch 127| loss: 39.4898 | valid_rmse: 6.570779800415039|  0:02:16s\nepoch 128| loss: 39.31031| valid_rmse: 6.591670036315918|  0:02:17s\nepoch 129| loss: 39.18421| valid_rmse: 6.5831098556518555|  0:02:18s\nepoch 130| loss: 39.1828 | valid_rmse: 6.5801801681518555|  0:02:19s\nepoch 131| loss: 39.0155 | valid_rmse: 6.575660228729248|  0:02:20s\nepoch 132| loss: 38.72388| valid_rmse: 6.569739818572998|  0:02:21s\nepoch 133| loss: 38.50544| valid_rmse: 6.5944600105285645|  0:02:23s\nepoch 134| loss: 38.91722| valid_rmse: 6.599470138549805|  0:02:24s\nepoch 135| loss: 38.8191 | valid_rmse: 6.575540065765381|  0:02:25s\nepoch 136| loss: 38.47973| valid_rmse: 6.586730003356934|  0:02:26s\nepoch 137| loss: 38.40577| valid_rmse: 6.547389984130859|  0:02:27s\nepoch 138| loss: 38.54077| valid_rmse: 6.550189971923828|  0:02:28s\nepoch 139| loss: 38.3472 | valid_rmse: 6.566649913787842|  0:02:29s\nepoch 140| loss: 38.2527 | valid_rmse: 6.568039894104004|  0:02:30s\nepoch 141| loss: 37.98002| valid_rmse: 6.559579849243164|  0:02:31s\nepoch 142| loss: 37.97332| valid_rmse: 6.558879852294922|  0:02:32s\nepoch 143| loss: 38.15618| valid_rmse: 6.529300212860107|  0:02:33s\nepoch 144| loss: 38.28458| valid_rmse: 6.567460060119629|  0:02:34s\nepoch 145| loss: 37.92368| valid_rmse: 6.554259777069092|  0:02:35s\nepoch 146| loss: 37.66446| valid_rmse: 6.560050010681152|  0:02:36s\nepoch 147| loss: 37.61826| valid_rmse: 6.5429301261901855|  0:02:38s\nepoch 148| loss: 37.78195| valid_rmse: 6.531439781188965|  0:02:39s\nepoch 149| loss: 37.60384| valid_rmse: 6.572480201721191|  0:02:40s\nStop training because you reached max_epochs = 150 with best_epoch = 143 and best_valid_rmse = 6.529300212860107\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### Evaluar el modelo en validaci√≥n\n\nUna vez entrenado el modelo final, lo probamos contra el conjunto de validaci√≥n. Con la funci√≥n de evaluaci√≥n calculo el error en `x` y en `y` (RMSE y MAE) y tambi√©n la distancia euclidiana promedio entre la posici√≥n real y la predicci√≥n.\n\nEstas m√©tricas nos dan una idea clara de qu√© tan bien est√° funcionando TabNet antes de usarlo para hacer predicciones sobre el conjunto de test que va a Kaggle.\n","metadata":{}},{"cell_type":"code","source":"# ===== PARTE 4 ‚Äì Paso 3: Evaluar el modelo en validaci√≥n =====\n\ny_pred_valid = final_model.predict(X_valid_tab)\n\nmetrics_valid = evaluate_predictions(\n    y_valid_tab,\n    y_pred_valid,\n    split_name=\"Validation (TabNet)\"\n)\n\nmetrics_valid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:48:58.608622Z","iopub.execute_input":"2025-12-04T03:48:58.609182Z","iopub.status.idle":"2025-12-04T03:48:58.744985Z","shell.execute_reply.started":"2025-12-04T03:48:58.609157Z","shell.execute_reply":"2025-12-04T03:48:58.744306Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüìä VALIDATION (TABNET) SET EVALUATION\n================================================================================\n\nüéØ POSITION ACCURACY:\n   X-coordinate:\n      RMSE: 6.786 yards\n      MAE:  4.856 yards\n\n   Y-coordinate:\n      RMSE: 6.262 yards\n      MAE:  4.505 yards\n\nüìè EUCLIDEAN DISTANCE:\n   Mean:   7.366 yards\n   Median: 6.049 yards\n   Std:    5.568 yards\n   Min:    0.044 yards\n   Max:    38.139 yards\n\nüìä DISTANCE PERCENTILES:\n   25th percentile: 3.195 yards\n   50th percentile: 6.049 yards\n   75th percentile: 10.024 yards\n   90th percentile: 14.888 yards\n   95th percentile: 18.372 yards\n   99th percentile: 25.882 yards\n\nüéØ ACCURACY BUCKETS:\n   Within  1 yards:    359 ( 3.90%)\n   Within  2 yards:  1,189 (12.92%)\n   Within  5 yards:  3,847 (41.79%)\n   Within 10 yards:  6,892 (74.87%)\n   Within 15 yards:  8,306 (90.23%)\n   Within 20 yards:  8,852 (96.17%)\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'x_rmse': 6.78568,\n 'y_rmse': 6.262428,\n 'x_mae': 4.8563957,\n 'y_mae': 4.5050354,\n 'mean_distance': 7.366203,\n 'median_distance': 6.0489078,\n 'distances': array([4.5301323, 4.8788433, 1.5224569, ..., 5.780381 , 1.5988973,\n        6.916272 ], dtype=float32)}"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"### Predicciones para test y creaci√≥n del archivo de Kaggle\n\nEn este √∫ltimo tramo utilizamos el modelo entrenado para predecir `x` e `y` en el conjunto de test. A partir de esas predicciones y de la metadata de cada ejemplo, construye el archivo `submission_tabnet.csv` en el formato que exige Kaggle.\n\nCada fila del archivo representa un jugador en un frame espec√≠fico de una jugada, con su identificador completo y las coordenadas estimadas. Este es el archivo que se sube a la plataforma para obtener el score del concurso.\n","metadata":{}},{"cell_type":"code","source":"# ===== PARTE 4 ‚Äì Paso 4: Predicciones TabNet sobre el conjunto de test =====\n\ny_pred_test = final_model.predict(X_test_tab)\n\nprint(\"Shape de predicciones para test:\", y_pred_test.shape)\nprint(\"Primeras 5 predicciones (x, y):\")\nprint(y_pred_test[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:49:19.818305Z","iopub.execute_input":"2025-12-04T03:49:19.818573Z","iopub.status.idle":"2025-12-04T03:49:19.850033Z","shell.execute_reply.started":"2025-12-04T03:49:19.818554Z","shell.execute_reply":"2025-12-04T03:49:19.849501Z"}},"outputs":[{"name":"stdout","text":"Shape de predicciones para test: (1758, 2)\nPrimeras 5 predicciones (x, y):\n[[29.064602 19.565416]\n [40.05082  28.915821]\n [22.21519  26.289627]\n [28.086645 33.64907 ]\n [30.727045 33.807045]]\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# ===== PARTE 4 ‚Äì Paso 5: Funci√≥n para crear el submission con TabNet =====\n\ndef create_submission_tabnet(y_pred_test, test_targets_df, metadata_test, filename=\"submission_tabnet.csv\"):\n    \"\"\"\n    Construye el archivo de submission para Kaggle usando las predicciones de TabNet.\n    \n    - y_pred_test: np.array de shape (N_test_samples, 2) con [x_pred, y_pred]\n    - test_targets_df: DataFrame con columnas game_id, play_id, nfl_id, frame_id\n    - metadata_test: lista de diccionarios con game_id, play_id, nfl_id (uno por fila de X_test_tab)\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"CREANDO ARCHIVO DE SUBMISI√ìN (TabNet)\")\n    print(\"=\"*80)\n\n    # 1) Diccionario clave -> predicci√≥n\n    pred_dict = {}\n    for meta, pred in zip(metadata_test, y_pred_test):\n        key = (meta['game_id'], meta['play_id'], meta['nfl_id'])\n        pred_dict[key] = {\n            \"x\": float(pred[0]),\n            \"y\": float(pred[1]),\n        }\n\n    # 2) Recorrer test_targets_df y asignar la predicci√≥n correspondiente\n    submissions = []\n    for _, row in test_targets_df.iterrows():\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n\n        if key in pred_dict:\n            x_pred = pred_dict[key][\"x\"]\n            y_pred = pred_dict[key][\"y\"]\n        else:\n            # fallback en caso de que falte alguna combinaci√≥n\n            x_pred = 60.0\n            y_pred = 26.65\n\n        submissions.append({\n            \"id\": f\"{row['game_id']}_{row['play_id']}_{row['nfl_id']}_{row['frame_id']}\",\n            \"x\": x_pred,\n            \"y\": y_pred,\n        })\n\n    submission_df = pd.DataFrame(submissions)\n    submission_df.to_csv(filename, index=False)\n    print(f\"Archivo de submission guardado como: {filename}\")\n    print(\"\\nVista previa de las primeras filas:\")\n    print(submission_df.head())\n\n    return submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:49:32.209984Z","iopub.execute_input":"2025-12-04T03:49:32.210265Z","iopub.status.idle":"2025-12-04T03:49:32.217190Z","shell.execute_reply.started":"2025-12-04T03:49:32.210244Z","shell.execute_reply":"2025-12-04T03:49:32.216550Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# ===== PARTE 4 ‚Äì Paso 6: Generar submission.csv para Kaggle =====\n\nsubmission_df = create_submission_tabnet(\n    y_pred_test=y_pred_test,\n    test_targets_df=test_targets_df,\n    metadata_test=metadata_test,\n    filename=\"submission_tabnet.csv\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T03:49:35.170063Z","iopub.execute_input":"2025-12-04T03:49:35.170782Z","iopub.status.idle":"2025-12-04T03:49:35.439422Z","shell.execute_reply.started":"2025-12-04T03:49:35.170758Z","shell.execute_reply":"2025-12-04T03:49:35.438807Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCREANDO ARCHIVO DE SUBMISI√ìN (TabNet)\n================================================================================\nArchivo de submission guardado como: submission_tabnet.csv\n\nVista previa de las primeras filas:\n                      id          x          y\n0  2024120805_74_54586_1  31.701832  16.473963\n1  2024120805_74_54586_2  31.701832  16.473963\n2  2024120805_74_54586_3  31.701832  16.473963\n3  2024120805_74_54586_4  31.701832  16.473963\n4  2024120805_74_54586_5  31.701832  16.473963\n","output_type":"stream"}],"execution_count":32}]}